Machine Learning - Study Notes
===============================

1. INTRODUCTION TO MACHINE LEARNING
------------------------------------
Machine Learning (ML) is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. Instead of following strict rules, ML systems improve their performance through experience.

Key Concepts:
- Training Data: Historical data used to teach the model
- Features: Input variables used for predictions
- Labels: Output variables (in supervised learning)
- Model: Mathematical representation learned from data
- Prediction: Output generated by the model for new data

2. TYPES OF MACHINE LEARNING
-----------------------------

Supervised Learning:
Supervised learning uses labeled data to train models. The algorithm learns to map inputs to outputs based on example input-output pairs.

Examples:
- Classification: Predicting categories (spam/not spam, cat/dog)
- Regression: Predicting continuous values (house prices, temperature)

Common algorithms: Linear Regression, Logistic Regression, Decision Trees, Random Forest, Support Vector Machines (SVM), Neural Networks

Unsupervised Learning:
Unsupervised learning finds patterns in unlabeled data. The algorithm discovers hidden structures without predefined outputs.

Examples:
- Clustering: Grouping similar data points (customer segmentation)
- Dimensionality Reduction: Reducing features while preserving information (PCA)
- Anomaly Detection: Identifying unusual patterns

Common algorithms: K-Means, DBSCAN, Hierarchical Clustering, PCA, Autoencoders

Reinforcement Learning:
An agent learns to make decisions by interacting with an environment. It receives rewards or penalties based on actions taken.

Examples:
- Game playing (Chess, Go, video games)
- Robotics control
- Recommendation systems

Key concepts: Agent, Environment, State, Action, Reward, Policy

3. LINEAR REGRESSION
--------------------
Linear regression is a supervised learning algorithm for predicting continuous values. It models the relationship between input features and output as a linear equation.

Formula: y = mx + b
Where:
- y: predicted output
- m: slope (weight)
- x: input feature
- b: intercept (bias)

For multiple features: y = w1*x1 + w2*x2 + ... + wn*xn + b

Training Process:
1. Initialize weights randomly
2. Calculate predictions
3. Compute error (loss) using Mean Squared Error (MSE)
4. Update weights using gradient descent
5. Repeat until convergence

Example Use Case:
Predicting house prices based on features like square footage, number of bedrooms, location, and age.

4. LOGISTIC REGRESSION
----------------------
Despite its name, logistic regression is used for classification problems. It predicts the probability of an instance belonging to a particular class.

Key Difference from Linear Regression:
- Uses sigmoid activation function to output probabilities between 0 and 1
- Outputs are interpreted as class probabilities

Sigmoid Function: σ(z) = 1 / (1 + e^(-z))

Applications:
- Binary classification (yes/no, true/false)
- Medical diagnosis (disease present/absent)
- Email spam detection
- Customer churn prediction

Decision Boundary:
- If probability > 0.5: Predict class 1
- If probability ≤ 0.5: Predict class 0

5. DECISION TREES
-----------------
Decision trees are versatile algorithms that can handle both classification and regression tasks. They learn decision rules from data features.

Structure:
- Root Node: Starting point with entire dataset
- Internal Nodes: Represent decisions based on features
- Branches: Outcomes of decisions
- Leaf Nodes: Final predictions

How It Works:
1. Select best feature to split data (using metrics like Gini impurity or information gain)
2. Create branches for different values
3. Recursively repeat for subsets
4. Stop when stopping criteria met (max depth, min samples, etc.)

Advantages:
- Easy to understand and visualize
- Handles both numerical and categorical data
- Requires little data preprocessing
- Non-linear relationships captured naturally

Disadvantages:
- Prone to overfitting
- Can be unstable with small data changes
- Biased toward dominant classes

6. RANDOM FOREST
----------------
Random Forest is an ensemble learning method that combines multiple decision trees to improve prediction accuracy and reduce overfitting.

Key Idea:
Train many decision trees on random subsets of data and features, then aggregate their predictions.

Process:
1. Create multiple bootstrap samples (random sampling with replacement)
2. Train decision tree on each sample
3. Each tree uses random subset of features at each split
4. Aggregate predictions:
   - Classification: Majority voting
   - Regression: Average of predictions

Advantages:
- Reduces overfitting compared to single decision tree
- Handles high-dimensional data well
- Provides feature importance rankings
- Robust to outliers and noise

Hyperparameters:
- n_estimators: Number of trees
- max_depth: Maximum depth of trees
- min_samples_split: Minimum samples to split node
- max_features: Number of features to consider for splits

7. NEURAL NETWORKS
------------------
Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) organized in layers.

Architecture:
- Input Layer: Receives input features
- Hidden Layers: Process and transform data
- Output Layer: Produces final predictions

Components:
- Weights: Connection strengths between neurons
- Biases: Shift activation functions
- Activation Functions: Introduce non-linearity (ReLU, Sigmoid, Tanh)

Forward Propagation:
1. Input data flows through network
2. Each neuron computes weighted sum of inputs
3. Activation function applied
4. Output passed to next layer

Backpropagation:
Algorithm for training neural networks by updating weights based on error gradients.

1. Calculate loss (error)
2. Compute gradients using chain rule
3. Update weights in reverse order (output to input)
4. Repeat for multiple epochs

Deep Learning:
Neural networks with many hidden layers (deep networks) capable of learning complex patterns.

Applications:
- Image recognition
- Natural language processing
- Speech recognition
- Game playing

8. MODEL EVALUATION
-------------------
Evaluating model performance is crucial for understanding effectiveness and generalization.

Classification Metrics:

Accuracy: (True Positives + True Negatives) / Total Predictions
- Simple but can be misleading with imbalanced data

Precision: True Positives / (True Positives + False Positives)
- Of predicted positives, how many are actually positive?

Recall (Sensitivity): True Positives / (True Positives + False Negatives)
- Of actual positives, how many did we identify?

F1-Score: 2 * (Precision * Recall) / (Precision + Recall)
- Harmonic mean of precision and recall

Confusion Matrix:
Visual representation showing correct and incorrect predictions for each class.

Regression Metrics:

Mean Squared Error (MSE): Average of squared differences
MSE = (1/n) * Σ(actual - predicted)²

Root Mean Squared Error (RMSE): Square root of MSE
- Same units as target variable

Mean Absolute Error (MAE): Average of absolute differences
MAE = (1/n) * Σ|actual - predicted|

R² Score (Coefficient of Determination):
- Proportion of variance explained by model
- Ranges from 0 to 1 (higher is better)

9. OVERFITTING AND UNDERFITTING
--------------------------------
Overfitting:
Model performs well on training data but poorly on new data. It memorizes noise instead of learning patterns.

Signs:
- High training accuracy, low test accuracy
- Model too complex for data

Solutions:
- Collect more training data
- Reduce model complexity
- Use regularization (L1/L2)
- Apply dropout (for neural networks)
- Cross-validation
- Early stopping

Underfitting:
Model is too simple to capture underlying patterns.

Signs:
- Low training and test accuracy
- High bias

Solutions:
- Increase model complexity
- Add more features
- Reduce regularization
- Train longer

Bias-Variance Tradeoff:
- High Bias: Model too simple (underfitting)
- High Variance: Model too complex (overfitting)
- Goal: Balance between bias and variance

10. CROSS-VALIDATION
--------------------
Cross-validation is a technique to assess model performance and prevent overfitting by using multiple train-test splits.

K-Fold Cross-Validation:
1. Divide data into K equal parts (folds)
2. Train on K-1 folds, test on remaining fold
3. Repeat K times, each fold used as test set once
4. Average performance across all folds

Benefits:
- Better use of limited data
- More reliable performance estimate
- Reduces variance in evaluation
- Helps detect overfitting

Common K values: 5 or 10

Stratified K-Fold:
Maintains class distribution in each fold (important for imbalanced datasets)

Leave-One-Out Cross-Validation (LOOCV):
K = number of samples (extreme case, computationally expensive)

11. FEATURE ENGINEERING
-----------------------
Feature engineering is the process of creating, transforming, and selecting features to improve model performance.

Techniques:

1. Feature Scaling:
   - Normalization: Scale to [0,1] range
   - Standardization: Zero mean, unit variance
   - Important for algorithms sensitive to scale (SVM, Neural Networks, KNN)

2. Feature Encoding:
   - One-Hot Encoding: Convert categories to binary columns
   - Label Encoding: Assign numeric labels to categories
   - Target Encoding: Replace category with target mean

3. Feature Creation:
   - Polynomial features: x, x², x³
   - Interaction features: x1 * x2
   - Domain-specific features: ratios, aggregations

4. Feature Selection:
   - Remove irrelevant or redundant features
   - Methods: Correlation analysis, Recursive Feature Elimination (RFE), Feature importance from tree models

5. Handling Missing Data:
   - Imputation: Fill with mean, median, mode, or predicted values
   - Deletion: Remove rows or columns with missing values
   - Indicator variables: Add binary flag for missingness

12. GRADIENT DESCENT
--------------------
Gradient descent is an optimization algorithm used to minimize loss functions by iteratively adjusting model parameters.

Basic Idea:
Move in the direction opposite to the gradient (steepest descent) to find the minimum of the loss function.

Algorithm:
1. Initialize parameters randomly
2. Calculate gradient (partial derivatives of loss)
3. Update parameters: θ = θ - α * ∇L
   - θ: parameters
   - α: learning rate
   - ∇L: gradient of loss

4. Repeat until convergence

Variants:

Batch Gradient Descent:
- Uses entire dataset for each update
- Slow but accurate
- Guaranteed convergence for convex functions

Stochastic Gradient Descent (SGD):
- Uses one sample for each update
- Fast but noisy
- Can escape local minima

Mini-Batch Gradient Descent:
- Uses small batch of samples
- Balance between speed and accuracy
- Most commonly used in practice

Learning Rate:
- Too small: Slow convergence
- Too large: May overshoot minimum, diverge
- Adaptive methods: Adam, RMSprop, AdaGrad

13. CLUSTERING ALGORITHMS
--------------------------
Clustering groups similar data points without predefined labels.

K-Means Clustering:
1. Choose K (number of clusters)
2. Initialize K centroids randomly
3. Assign each point to nearest centroid
4. Update centroids as mean of assigned points
5. Repeat steps 3-4 until convergence

Choosing K: Elbow method, Silhouette score

Hierarchical Clustering:
- Agglomerative (bottom-up): Start with individual points, merge similar clusters
- Divisive (top-down): Start with one cluster, split recursively

Dendrogram: Tree diagram showing cluster hierarchy

DBSCAN (Density-Based Spatial Clustering):
- Groups points in high-density regions
- Can find arbitrarily-shaped clusters
- Identifies outliers as noise points

Parameters:
- eps: Maximum distance between neighbors
- min_samples: Minimum points to form cluster

14. SUPPORT VECTOR MACHINES (SVM)
----------------------------------
SVM is a powerful algorithm for classification and regression that finds the optimal hyperplane separating classes.

Key Concepts:

Hyperplane: Decision boundary separating classes
Support Vectors: Data points closest to hyperplane
Margin: Distance between hyperplane and nearest points

Objective: Maximize margin while correctly classifying points

Kernel Trick:
Transform data into higher dimensions to make it linearly separable.

Common Kernels:
- Linear: No transformation
- Polynomial: (x·y + c)^d
- RBF (Radial Basis Function): exp(-γ||x-y||²)
- Sigmoid: tanh(αx·y + c)

Soft Margin:
Allow some misclassifications with penalty parameter C:
- Large C: Fewer misclassifications, smaller margin
- Small C: More misclassifications, larger margin

Applications:
- Text classification
- Image recognition
- Bioinformatics

15. DIMENSIONALITY REDUCTION
-----------------------------
Reducing number of features while preserving important information.

Principal Component Analysis (PCA):
- Finds orthogonal axes (principal components) that capture maximum variance
- Projects data onto these components
- Keeps top K components explaining most variance

Process:
1. Standardize data
2. Compute covariance matrix
3. Calculate eigenvectors and eigenvalues
4. Sort by eigenvalues (descending)
5. Project data onto top K eigenvectors

Benefits:
- Visualization (reduce to 2D or 3D)
- Noise reduction
- Faster training
- Avoid curse of dimensionality

t-SNE (t-Distributed Stochastic Neighbor Embedding):
- Non-linear dimensionality reduction
- Excellent for visualization
- Preserves local structure
- Computationally expensive

Use Cases:
- Visualizing high-dimensional data
- Feature extraction
- Exploratory data analysis